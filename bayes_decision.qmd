---
title: "Optimizing Risk Decisions with Imperfect Data"
author: "Joel Anderson"
format: docx
editor: visual
date: "`r format(Sys.time(), '%B %d, %Y')`"
---

## Abstract

In any integrity management program, there are always competing alternatives for any decision, such as to dig or not to dig an anomaly, replace or not replace. If perfect information were always available similar to a math problem where everything is given except the answer, risk engineers would be an unnecessary expense. Barring perfect information, the alternative option could be an exhaustive corpus of data with frequencies of every outcome and condition combination. However, in all but the most trivial cases, neither of these exist, and the engineer is dealt partial, imperfect information where the true state of nature is uncertain. To deal with this uncertainty in everyday life people develop heuristics, mental shortcuts that allow us to process this information with minimal effort and time. But when the probabilities are imprecise, the decisions based on these shortcuts can be fraught with biases and fallacies. However, all decisions carry some amount of risk which is dependent on the (uncertain) true state of nature, and the potential loss associated with each action.

This paper will demonstrate an innovative application of decision theory that incorporates existing knowledge with observations to quantify the trade-offs of competing alternatives in an integrity management program to arrive at a decision that minimizes the risk based on the state of knowledge at that given time.

# Introduction

"The information you have is not the information you want. The information you want is not the information you need. The information you need is not the information you can obtain. The information you can obtain costs more than you want to pay"[^1]. Every decision, especially in an integrity management program involves reasoning with imperfect or partial information. You will always be at a deficit of knowledge and the option of no decision is a decision onto itself to do nothing. We desire for complete information but that will never be coming to the rescue of any decision that has an element of limitations on time and money. Any person that has been part of an integrity management program has had conversations or been in meetings discussing something that is *probably* not a integrity threat but it *could* be depending on the circumstances. But nobody can say definitively one way or the other. The decision makers must weigh the incomplete information that they have against what true state of the anomaly could be and try to decide the best action with their state of knowledge at the time. This paper will discuss the basics of decision theory and how it can be applied to an integrity management program.

[^1]: Peter L. Bernstein, Against the Gods: The Remarkable Story of Risk.

# Decision Theory

Decision theory uses economic and statistical approaches for studying an individual's rational decision in the face of unknown and uncertain conditions. It has its origin in 17th century with Blaise Pasacal's wager and refined in WWII with the need to interpret noisy signals and decide what action to take. The motivation behind decision theory is that without a systematic framework, two people of equal ability can be shown the same data can come to different conclusions. Why is that? It is because people though they think of themselves as rational, are really very irrational oftentimes when decisions must be made. That is why it is necessary to have a way to systematically reason with the data at hand. It is important to realize that a decision with a bad outcome is not necessarily the wrong decision and a decision with a favorable outcome is not necessarily the right decision. For example: betting one's paycheck on a single spin of the roulette wheel and winning is a positive outcome but still the wrong decision since based on what was known ahead of time the more likely outcome was the loss of your paycheck. Likewise a negative outcome does not equate to a bad decision due to the fact that you don't have the outcome available prior to the decision to consider. Since the outcome is not known ahead of time, it is not meaningful to use terms such as "right" or "wrong" when discussing decisions but rather rational or irrational since that is only what is able to be judged prior to the outcome. Right and wrong can only be judged in light of the outcome therefore the best that can be hoped for is rational or optimal decision based on our preferences and available information. One has to reason against the information at hand and can't wait for the outcome to decide the course of action. A rational decision is one that is best given the alternatives available, the information possessed, and the preferences of the decision maker.

Any decision involves the following things:

-   The uncertain state of nature ($\omega_i$)

-   Possible actions ($a_i$)

-   The consequences (loss) for each combination of state of nature and course of action decided ($\lambda_i$)

These are all combined with an observation ($x$) with some level of uncertainty associated with it. When facing a decision problem, the decision maker aims to choose the *action* that will have the best *outcome*. But the outcome of each act depends on the *state* of nature, which the true state is unknown to the decision maker. Decision theory is a means to describe the decision maker's state of knowledge at that time and minimize the risk given the knowledge. So it is not only necessary to compute the probability of the state of nature without having witnessed it directly, but decide on what the course of action needs to be.

# Fallacies in Reasoning

Laplace in his *Essai Philosophique sur les ProbabilitÂ´es* discusses an urn containing 1,000 balls in which 1 is white and 999 are black. A person whom is known to be truthful $\frac{9}{10}$ of the time draws a ball at random and declares it to be white. Without thinking, most people will say the probability of it being white is now 0.9 because the witness is truthful 9 times out of 10. However this reasoning is wrong. While the posterior odds (after declaring it white) are 9 times higher than prior to the declaration, the assumption that the probability of white after the declaration equal to 0.9 would require pretending that you didn't know anything prior to drawing the ball. The true probability of it being white is about 0.009. By ignoring prior knowledge, the assumed probability is one hundred times higher than the true probability.

The problem with the balls in the urn is what is known as the base-rate fallacy, it ignores the prior probability of it being white ($\frac{1}{1000}$ ). If we take the basic definition of probability as the ratio of favorable outcomes to the total number of possible outcomes and *w* is the number of white balls in the urn and *b* the number of black. Then the probability of white is $\frac{w}{w+b}$ and if after declaring the drawn ball to be white, the odds of white have increased by a factor of 9, the posterior probability of white becomes $\frac{9}{9+b}$ or $\frac{9}{1008} \approx 0.9\%$ and as the number of black balls is increased the probability of the witness being deceptive approaches certainty. Ignoring prior knowledge in any decision is almost certainly at your own peril.

Since all decision involve some level of uncertainty, the process must involve probability as a tool to reason with.

# Prior Probability

In the discussion of decision theory there is a need to first describe the concept of prior probability. This is often sneered at in some circles as injecting subjectivity into the calculus of statistics. But as is seen in the problem of the balls in the urn it can't be ignored. But the difficulty in most integrity management decisions, and most decisions in general, is the population of the "urn" is not known ahead of time and there is no comprehensive database of all possible conditions and outcomes that one can query to see how often something has occurred in the past when combined with a certain observation. If I was to make an extraordinary claim that I had a vertical jump exceeding that of the average NBA player most people would dismiss that out of hand without knowing anything about the rate of truthfulness of my statements in the past or physical ability. Why? Do they have a historical dataset of vertical jump capabilities for engineers? No, of course not, they reasoned from logic and experience (priors) this is an extraordinary claim and formed a very small prior probability of being true even if they didn't put a specific number on it. Because of the small prior placed on the extraordinary claim it would require a proportionally large evidence to overcome it and convince them that it is true. In any decision problem, you may not know the exact likelihood of the condition but you always have some frame of reference about the size of the problem based on general knowledge and prior experience.

Such as it is in everyday life, people form and use prior probabilities without realizing it. Since the true state of nature is always uncertain it is necessary to form a prior probability for each of the finite possible states of nature that are likely to exist. This list of possible states needs to be exhaustive of all possible conditions that are under consideration and since there is only one true state, the sum of all prior probabilities must sum to one. If $\omega_1, \ldots, \omega_n$ are the possible states of nature that exist with probability $P(\omega_1), \ldots, P(\omega_n)$ then $\sum_{i =1}^{n}P(\omega_i) = 1$.

# Actions

Now that we have formed a prior probability for each possible state of nature being considered, it is necessary to determine what are the different actions that can be taken. Just as each classification of the state of nature needs to be defined, the list of all possible actions needs to be defined. For example if the thing under consideration is an anomaly from a ILI run, there is effectively two possible actions either dig it up to examine and remediate it or not. If a more refined analysis is desired, the two options could be split into three. Either dig it immediately, wait for the results of other digs to decide on whether to examine it, or not dig it all.

# Loss Function

The third necessary preliminary step is the define the loss function. This is the loss (or gain) that is expected if we decide to take action $\alpha_i$ which is also dependent on the true state of nature is $\omega_i$. Since the loss is dependent on the true state of nature and the decided action the loss is a function of both, $\lambda_i(\alpha_i, \ \omega_j)$. Putting it another way, it is the loss suffered by taking action $\alpha_i$ when the true state of nature is $\omega_j$, this is known as the conditional loss. For example, the loss expected if the true state of nature of an ILI anomaly is that it would rupture before the next reassessment and based on the ILI call, it was decided not to examine it or $\lambda(\alpha = no \ dig, \ \omega = Rupture)$. If only the was the criteria for a decision then we would simply find the maximum loss then choose the action which minimizes this loss, this is known as the minmax criteria. This would be sufficient if all states of nature were equally likely or if nature somehow always chose the state that causes the maximum loss. If the worst case is always the assumed state of true nature then this whole exercise is moot. But that pessimistic assumption is no more valid than the eternal optimist that assumes that the negative outcome can't happen to them.

# Risk

Since not all outcomes and states of nature are equally likely, it is necessary to consider not just the loss but the likelihood of the loss as well. Therefore the conditional risk is the product of the loss for action $i$ and the state of nature is $j$ and the probability of state $j$ given the observation $x$ summed over all possible classes.  

$$ 
R(\alpha_i|x) = \sum_{j = 1}^{c}\lambda(\alpha_i|\omega_j)P(\omega_j|x)
$$
The risk is minimized by by selecting the action that minimizes the conditional risk. When this criteria is applied it will produce the optimal decision that has the least amount of risk. This will be more clear when it is applied to a more concrete example.

# Example
In this hypothetical scenario there is an anomaly from an ILI run and because of the tool tolerance and experience with inaccuracies in the past there is uncertainty in the true condition of the anomaly.

## Decision Process  
The first step in any decision analysis is to enumerate all possible states of nature under consideration. In the case of an ILI anomaly there are only three mutually exclusive states. 1) it will be safe until the next reassessment, 2) it will leak before the next reassessment or 3) the anomaly will rupture before it is reassessed. There's actually another possible state that the anomaly has already failed. But if that was the case, the decision has effectively been made for you and the discussion on what to do is moot. Therefore it is assumed that is not the case and is excluded from the analysis.

## Priors  
After defining the possible states of nature it is necessary to assign a probability to each possible state based on the observation (the ILI call) and prior knowledge of other ILI anomalies examined in past ILI runs. The prior probabilities assigned in this case are as follows. The decision maker is 90% certain that is safe until reassessment, 9% it would leak and 1% chance that it would rupture. The prior probabilities $P(\omega_j) = 0.9, \ 0.09, \ 0.01$ for safe, leak and rupture respectively. Since we are enumerating the exhaustive list of possible states of nature, teh prior probabilities must sum to 1.

## Call - Conditionals 
The next step is define the call conditionals. In reality ILI accuracy is not a yes/no, binary answer but a continuous gradation of potential error but for the sake of example simplicity, we'll limit it to two conditions, the ILI was accurate or inaccurate (however you want to define those). Then for each of the possible states of nature (Safe, Leak and Rupture) a probability is assigned to each of the call conditions, $ P(x|\omega_j)$. The outcome will be a matrix of probabilities that is the number of states (3 in this case) wide by the number of conditionals long (2 in this case). In this example is might look something like this:

```{r}
cnames <- c('P(x|wj)', 'Safe', 'Leak', 'Rupture')
p <- c("accurate", "inaccurate")
ps <- c(0.9,0.1)
pl <- c(0.02,0.98)
pr <- c(0.01, 0.99)

pcc <- tibble(p,ps, pl,pr)
names(pcc) <- cnames

flextable::flextable(pcc)

```

For instance the probability that the ILI was accurate if the true state of the anomaly is that it is safe is 0.9 and 0.1 if the tool was inaccurate. Then the same thing is done for the other states of nature and call-conditional combinations. It is important to realize that these are the probability of the call conditional given the state of nature. It is the probability that the tool was accurate or inaccurate if the true state of nature was safe leak or rupture. The 0.9 is not the probability that the anomaly is safe if the tool was accurate, not the same. For example if the observer looks outside and sees that the sidewalk is wet. This might have been caused by rain or sprinklers so the two conditionals base on observing the wet sidewalk are P(rain|wet) and P(sprinklers|wet). The Probability of wet sidewalk if it rained or sprinklers went off ~ 100% but that doesn't help the problem of deciding the cause of the wet sidewalk. For each state of nature, the conditionals must sum to 1.
