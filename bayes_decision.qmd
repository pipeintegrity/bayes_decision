---
title: "Optimizing Risk Decisions with Imperfect Data"
author: "Joel Anderson"
format: docx
editor: visual
date: "`r format(Sys.time(), '%B %d, %Y')`"
---

## Abstract

In any integrity management program, there are always competing alternatives for any decision, such as to dig or not to dig an anomaly, replace or not replace. If perfect information were always available similar to a math problem where everything is given except the answer, risk engineers would be an unnecessary expense. Barring perfect information, the alternative option could be an exhaustive corpus of data with frequencies of every outcome and condition combination. However, in all but the most trivial cases, neither of these exist, and the engineer is dealt partial, imperfect information where the true state of nature is uncertain. To deal with this uncertainty in everyday life people develop heuristics, mental shortcuts that allow us to process this information with minimal effort and time. But when the probabilities are imprecise, the decisions based on these shortcuts can be fraught with biases and fallacies. However, all decisions carry some amount of risk which is dependent on the (uncertain) true state of nature, and the potential loss associated with each action.

This paper will demonstrate an innovative application of decision theory that incorporates existing knowledge with observations to quantify the tradeoffs of competing alternatives in an integrity management program to arrive at a decision that minimizes the risk based on the state of knowledge at that given time.

# Introduction

"The information you have is not the information you want. The information you want is not the information you need. The information you need is not the information you can obtain. The information you can obtain costs more than you want to pay"[^1]. Every decision, especially in an integrity management program involves reasoning with imperfect or partial information. You will always be at a deficit of knowledge and the option of no decision is a decision onto itself to do nothing. We desire for complete information but that will never be coming to the rescue of any decision that

[^1]: Peter L. Bernstein, Against the Gods: The Remarkable Story of Risk.

Two people of equal ability can be shown the same data can come to different conclusions. Why is that? It is because people though they think of themselves as rational are really very irrational. That is why it is necessary to have a way to systematically reason with the data at hand. It is important to realize that a decision with a bad outcome is not necessarily a bad decision and a decision with a favorable outcome is not necessarily a good decision. For example: betting a paycheck on a single spin of the roulette wheel and winning is a positive outcome but still a bad decision. Likewise a negative outcome does not equate to a bad decision due to the fact that you don't have the outcome prior to the decision to consider. One has to reason against the information at hand and can't wait for the outcome to decide the course of action. A person has to reason from the information that have at the time. A person with better information may make better decisions but this doesn't mean that person that had less information made a bad decision.

Any decision involves the following things:

-   The (usually) uncertain state of nature ($S_i$)

-   Possible actions ($a_i$)

-   The potential consequences for each combination of selected state and outcome ($\lambda_i$)

These are all combined with an observation with some level of uncertainty associated with it.

# Fallacies in Reasoning

Laplace in his *Essai Philosophique sur les ProbabilitÂ´es* discusses an urn containing 1,000 balls in which 1 is white and 999 are black. A person whom is known to be honest $\frac{9}{10}$ of the time draws a ball at random and declares it to be white. Without thinking, most people will say the probability of it being white is now 0.9 but this is wrong. While the posterior odds (after declaring it white) are 9 times higher than prior to the declaration, the assumption that the probability of white after the declaration equal to 0.9 would require pretending that you didn't know anything prior to drawing the ball. The true probability of it being white is about 0.009. By ignoring prior knowledge, the assumed probability is two magnitudes of order higher than the true probability. The assumption would cause a gross error of a factor of 100 in likelihood ball being white.

The problem with the balls in the urn is what is known as the base-rate fallacy, it ignores the prior probability of it being white ($\frac{1}{1000}$ ). If we take the basic definition of probability as the ratio of favorable outcomes to the total number of possible outcomes and *w* is the number of white balls in the urn and *b* the number of black. Then the probability of white is $\frac{w}{w+b}$ and if after declaring the drawn ball to be white the odds of white have increased by a factor of 9, the posterior probability of white becomes $\frac{9}{9+b}$ or $\frac{9}{1008} \approx 0.9\%$ and as the number of black balls is increased the probability of the witness being deceptive approaches certainty. Ignoring prior knowledge in any decision is almost certainly at your own peril.

Since all decision involve some level of uncertainty, the process must involve probability as a tool to reason with.

# Prior Probability

In the discussion of decision theory there is a need to first describe the concept of prior probability. This is often sneered at in some circles as injecting subjectivity into the calculus of statistics. But as is seen in the problem of the balls in the urn it can't be ignored. But the difficulty in most integrity management decisions, and most decision in general, is the population of the "urn" is not known ahead of time and there is no comprehensive database of all possible conditions and outcomes that one can query to see how often something has occurred in the past. If I was to make an extraordinary claim that I had a vertical jump exceeding that of the average NBA player most people would dismiss that out of hand without knowing anything about the rate of truthfulness of my statements in the past. Why? Do they have a historical dataset of vertical jump capabilities for integrity engineers? No, of course not, they reasoned from logic and experience this is an extraordinary claim and formed a very small prior probability of being true even if they didn't put a specific number on it.

Such as it is in everyday life
