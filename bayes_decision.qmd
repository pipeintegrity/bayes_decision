---
title: "Optimizing Risk Decisions with Imperfect Data"
author: "Joel Anderson"
format: docx
editor: visual
date: "`r format(Sys.time(), '%B %d, %Y')`"
---

## Abstract

In any integrity management program, there are always competing alternatives for any decision, such as to dig or not to dig an anomaly, replace or not replace. If perfect information were always available similar to a math problem where everything is given except the answer, risk engineers would be an unnecessary expense. Barring perfect information, the alternative option could be an exhaustive corpus of data with frequencies of every outcome and condition combination. However, in all but the most trivial cases, neither of these exist, and the engineer is dealt partial, imperfect information where the true state of nature is uncertain. To deal with this uncertainty in everyday life people develop heuristics, mental shortcuts that allow us to process this information with minimal effort and time. But when the probabilities are imprecise, the decisions based on these shortcuts can be fraught with biases and fallacies. However, all decisions carry some amount of risk which is dependent on the (uncertain) true state of nature, and the potential loss associated with each action.

This paper will demonstrate an innovative application of decision theory that incorporates existing knowledge with observations to quantify the trade-offs of competing alternatives in an integrity management program to arrive at a decision that minimizes the risk based on the state of knowledge at that given time.

# Introduction

"The information you have is not the information you want. The information you want is not the information you need. The information you need is not the information you can obtain. The information you can obtain costs more than you want to pay"[^1]. Every decision, especially in an integrity management program involves reasoning with imperfect or partial information. You will always be at a deficit of knowledge and the option of no decision is a decision onto itself to do nothing. We desire for complete information but that will never be coming to the rescue of any decision that has an element of limitations on time and money. Any person that has been part of an integrity management program has had conversations or been in meetings discussing something that is *probably* not a integrity threat but it *could* be depending on the circumstances. But nobody can say definitively one way or the other. The decision makers must weigh the incomplete information that they do have against what true state of the anomaly could be and try to decide the best action with their state of knowledge at the time. This paper will discuss the basics of decision theory and how it can be applied to an integrity management program.

[^1]: Peter L. Bernstein, Against the Gods: The Remarkable Story of Risk.

# Decision Theory

Decision theory uses economic and statistical approaches for studying an individual's rational decision in the face of unknown and uncertain conditions. The motivation behind decision theory is that without a systematic framework, two people of equal ability can be shown the same data can come to different conclusions. Why is that? It is because people though they think of themselves as rational, are really very irrational oftentimes when decisions must be made. That is why it is necessary to have a way to systematically reason with the data at hand. It is important to realize that a decision with a bad outcome is not necessarily the wrong decision and a decision with a favorable outcome is not necessarily the right decision. For example: betting one's paycheck on a single spin of the roulette wheel and winning is a positive outcome but still the wrong decision. Likewise a negative outcome does not equate to a bad decision due to the fact that you don't have the outcome available prior to the decision to consider. Since the outcome is not known ahead of time, it is not meaningful to use terms such as "right" or "wrong" when discussing decisions but rather rational or irrational since that is only what is able to be judged prior to the outcome. Right and wrong can only be judged in light of the outcome therefore the best that can be hoped for is rational or optimal decision based on our preferences and available information. One has to reason against the information at hand and can't wait for the outcome to decide the course of action. Somebody with better information may make a better decision but this doesn't mean that person that had less information made a bad decision.

Any decision involves the following things:

-   The uncertain state of nature ($\omega_i$)

-   Possible actions ($a_i$)

-   The consequences (loss) for each combination of state of nature and course of action decided ($\lambda_i$)

These are all combined with an observation ($x$) with some level of uncertainty associated with it. When facing a decision problem, the decision maker aims to choose the *action* that will have the best *outcome*. But the outcome of each act depends on the *state* of nature, which the true state is unknown to the decision maker. Decision theory is a means to describe the decision maker's state of knowledge at that time and minimize the risk given the knowledge. So it is not only necessary to compute the probability of the state of nature without having witnessed it directly, but decide on what the course of action needs to be.

# Fallacies in Reasoning

Laplace in his *Essai Philosophique sur les ProbabilitÂ´es* discusses an urn containing 1,000 balls in which 1 is white and 999 are black. A person whom is known to be truthful $\frac{9}{10}$ of the time draws a ball at random and declares it to be white. Without thinking, most people will say the probability of it being white is now 0.9 because the witness is truthful 9 times out of 10. However this reasoning is wrong. While the posterior odds (after declaring it white) are 9 times higher than prior to the declaration, the assumption that the probability of white after the declaration equal to 0.9 would require pretending that you didn't know anything prior to drawing the ball. The true probability of it being white is about 0.009. By ignoring prior knowledge, the assumed probability is one hundred times higher than the true probability.

The problem with the balls in the urn is what is known as the base-rate fallacy, it ignores the prior probability of it being white ($\frac{1}{1000}$ ). If we take the basic definition of probability as the ratio of favorable outcomes to the total number of possible outcomes and *w* is the number of white balls in the urn and *b* the number of black. Then the probability of white is $\frac{w}{w+b}$ and if after declaring the drawn ball to be white, the odds of white have increased by a factor of 9, the posterior probability of white becomes $\frac{9}{9+b}$ or $\frac{9}{1008} \approx 0.9\%$ and as the number of black balls is increased the probability of the witness being deceptive approaches certainty. Ignoring prior knowledge in any decision is almost certainly at your own peril.

Since all decision involve some level of uncertainty, the process must involve probability as a tool to reason with.

# Prior Probability

In the discussion of decision theory there is a need to first describe the concept of prior probability. This is often sneered at in some circles as injecting subjectivity into the calculus of statistics. But as is seen in the problem of the balls in the urn it can't be ignored. But the difficulty in most integrity management decisions, and most decision in general, is the population of the "urn" is not known ahead of time and there is no comprehensive database of all possible conditions and outcomes that one can query to see how often something has occurred in the past when combined with a certain observation. If I was to make an extraordinary claim that I had a vertical jump exceeding that of the average NBA player most people would dismiss that out of hand without knowing anything about the rate of truthfulness of my statements in the past or physical ablilty. Why? Do they have a historical dataset of vertical jump capabilities for engineers? No, of course not, they reasoned from logic and experience (priors) this is an extraordinary claim and formed a very small prior probability of being true even if they didn't put a specific number on it. Because of the small prior placed on the extraordinary claim it would require a proportionally large evidence to overcome it and convince them that it is true.

Such as it is in everyday life, people form and use prior probabilities without realizing it. Since the true state of nature is uncertain it is necessary to form a prior probability for each of the finite possible states of nature that are likely to exist. This list of possible states needs to be exhaustive of all possible conditions that are under consideration and since there is only one true state, the sum of all prior probabilities must sum to one. If $\omega_1, \ldots, \omega_n$ are the possible states of nature that exist with probability $P(\omega_1), \ldots, P(\omega_n)$ then $\sum_{i =1}^{n}P(\omega_i) = 1$.

# Actions

Now that we have formed a prior probability for each possible state of nature being considered, it is necessary to determine what are the different actions that can be taken. Just as each classification of the state of nature needs to be defined, the list of all possible actions needs to be defined. For example if the thing under consideration is an anomaly from a ILI run, there is effectively two possible actions either dig it up to examine and remediate it or not. If a more refined analysis is desired, the two options could be split into three. Either dig it immediately, wait for the results of other digs to decide on whether to examine it, or not dig it all.

# Loss Function

The third necessary preliminary step is the define the loss function. This is the loss (or gain) that is expected if we decide to take action $\alpha_i$ which is also dependent on the true state of nature is $\omega_i$. Since the loss is dependent on the true state of nature and the decided action the loss is a function of both, $\lambda_i(\alpha_i, \ \omega_i)$. For example the loss expected if the true state of nature of an ILI anomaly is that it would rupture before the next reassessment and based on the ILI call, it was decided not to examine it or $\lambda(\alpha = no \ dig, \ \omega = Rupture)$. If only the was the criteria for a decision then we would simply find the maximum loss then choose the action which minimizes this loss, this is known as the minmax criteria. This would be sufficient if all states of nature were equally likely or if nature somehow always chose the state that causes the maximum loss. If the worst case is always the assumed state of true nature then this whole exercise is moot. But that pessimistic assumption is no more valid than the eternal optimist that assumes that the negative outcome can't happen to them.
